{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:17:24.245918Z",
     "end_time": "2023-05-01T17:17:24.250672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:17:25.185233Z",
     "end_time": "2023-05-01T17:17:25.189191Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Similarity using SpaCy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T17:17:27.152333Z",
     "end_time": "2023-05-01T17:17:28.909969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041243947777828\n"
     ]
    }
   ],
   "source": [
    "# Load English model for tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "# Create documents\n",
    "doc1 = nlp(u'I love pets.')\n",
    "doc2 = nlp(u'I hate pets')\n",
    " \n",
    "# Find similarity\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Jaccard Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T17:17:33.000356Z",
     "end_time": "2023-05-01T17:17:33.003267Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(sent1, sent2):\n",
    "    \"\"\"Find text similarity using jaccard similarity\"\"\"\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    token1 = set(sent1.split())\n",
    "    token2 = set(sent2.split())\n",
    "     \n",
    "    # intersection between tokens of two sentences    \n",
    "    intersection_tokens = token1.intersection(token2)\n",
    "    \n",
    "    # Union between tokens of two sentences\n",
    "    union_tokens=token1.union(token2)\n",
    "    \n",
    "    # jaccard Similarity\n",
    "    sim_= float(len(intersection_tokens) / len(union_tokens))\n",
    "    return sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call function\n",
    "jaccard_similarity('I love pets.','I hate pets.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:17:33.834212Z",
     "end_time": "2023-05-01T17:17:33.842239Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cosine Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T17:17:35.391838Z",
     "end_time": "2023-05-01T17:17:35.395652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.33609693]\n",
      " [0.33609693 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Let's import text feature extraction TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import cosine_similarity metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "docs=['I love pets.','I hate pets.']\n",
    " \n",
    "# Initialize TfidfVectorizer object\n",
    "tfidf= TfidfVectorizer()\n",
    " \n",
    "# Fit and transform the given data\n",
    "tfidf_vector = tfidf.fit_transform(docs)\n",
    "\n",
    "# compute similarity using cosine similarity\n",
    "cos_sim=cosine_similarity(tfidf_vector, tfidf_vector)\n",
    "print(cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
